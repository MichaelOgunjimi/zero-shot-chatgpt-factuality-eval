# GPT-4o-mini Configuration for Tier 2
# Rate limits: 2,000 RPM, 2,000,000 TPM, 2,000,000 Batch queue limit

openai:
  models:
    primary: "gpt-4o-mini"
    fallbacks: ["gpt-4.1-mini", "o1-mini"]

  # Rate Limiting - Tier 2 for gpt-4o-mini
  rate_limits:
    requests_per_minute: 1800 # 90% of 2,000 RPM for safety margin
    tokens_per_minute: 1800000 # 90% of 2,000,000 TPM for safety margin
    requests_per_day: 288000 # Conservative daily estimate (1800 RPM * 60 * 24 * 0.1)
    concurrent_requests: 20 # Higher concurrency for Tier 2
    batch_queue_limit: 1800000 # 90% of 2,000,000 batch queue limit

  # Generation parameters optimized for gpt-4o-mini
  generation:
    temperature: 0.0
    max_tokens: 2048
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
    stop_sequences: []

  # Cost management for gpt-4o-mini
  cost_control:
    cost_per_1k_tokens:
      gpt-4o-mini:
        input: 0.00015 # $0.15 per 1M tokens
        output: 0.0006 # $0.60 per 1M tokens

  # Batch processing for gpt-4o-mini
  batch:
    enabled: true
    max_queue_size: 1800000 # 90% of batch queue limit
    processing_timeout: 86400
    cost_savings: 0.5
