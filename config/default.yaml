# Complete configuration for Factuality Evaluation Project
# MSc AI Thesis - University of Manchester

# Project metadata
project:
  name: "factuality-eval"
  version: "1.0.0"
  author: "Michael Ogunjimi"
  description: "Factuality evaluation for automatic text summarisation"
  thesis_deadline: "2025-08-15"

# Dataset configurations
datasets:
  cnn_dailymail:
    name: "cnn_dailymail"
    version: "3.0.0"
    sample_size: 1000  # Set to null for full dataset
    max_source_length: 1024
    min_summary_length: 10
    max_summary_length: 256
    splits: ["test"]
    preprocessing:
      remove_duplicates: true
      clean_text: true
      filter_by_length: true
      remove_urls: true
      remove_emails: true

  xsum:
    name: "xsum"
    sample_size: 1000
    max_source_length: 1024
    min_summary_length: 10
    max_summary_length: 128
    splits: ["test"]
    preprocessing:
      remove_duplicates: true
      clean_text: true
      filter_by_length: true
      remove_urls: true
      remove_emails: true

  summeval:
    name: "summeval"
    sample_size: null  # Use full dataset
    preprocessing:
      include_human_scores: true
      human_score_types: ["consistency", "coherence", "fluency", "relevance"]

  multi_news:
    name: "multi_news"
    sample_size: 500
    max_source_length: 2048  # Multi-document summaries are longer
    min_summary_length: 15
    max_summary_length: 512
    splits: ["test"]
    preprocessing:
      remove_duplicates: true
      clean_text: true
      filter_by_length: true

# Metric configurations
metrics:
  factcc:
    model_name: "salesforce/factcc"
    batch_size: 16
    max_length: 512
    device: "auto"  # auto, cuda, mps, cpu
    cache_dir: "${CACHE_DIR:./data/cache}"
    threshold: 0.5
    return_probabilities: true
    use_confidence_intervals: true

  rouge:
    rouge_types: ["rouge1", "rouge2", "rougeL"]
    use_stemmer: true
    use_aggregator: true
    bootstrap_confidence: true
    confidence_intervals: 0.95
    split_summaries: false
    bootstrap_samples: 1000

  bertscore:
    model_type: "bert-base-uncased"
    num_layers: 9
    verbose: false
    batch_size: 64
    rescale_with_baseline: true
    use_fast_tokenizer: true

  qags:
    question_generator:
      model_name: "facebook/bart-base"
      max_questions: 5
      batch_size: 8
      temperature: 0.7
      do_sample: true
    qa_model:
      model_name: "microsoft/deberta-base"
      batch_size: 16
      confidence_threshold: 0.1
    device: "auto"
    cache_dir: "${CACHE_DIR:./data/cache}"
    string_similarity_method: "jaccard"  # jaccard, bleu, rouge

  feqa:
    question_generator:
      model_name: "facebook/bart-base"
      temperature: 0.7
    qa_model:
      model_name: "microsoft/deberta-base"
      confidence_threshold: 0.1
    answer_extraction:
      mask_probability: 0.3
      max_answer_length: 50
      min_answer_length: 2
    device: "auto"
    cache_dir: "${CACHE_DIR:./data/cache}"

  # LLM-based evaluation configurations
  llm:
    # OpenAI models
    openai:
      enabled: true
      models:
        gpt4:
          model: "gpt-4"
          api_key: "${OPENAI_API_KEY}"
          temperature: 0.0
          max_tokens: 150
          timeout: 30
          rate_limit: 60  # requests per minute
          cost_per_1k_input: 0.03
          cost_per_1k_output: 0.06
        gpt4_turbo:
          model: "gpt-4-turbo"
          api_key: "${OPENAI_API_KEY}"
          temperature: 0.0
          max_tokens: 150
          timeout: 30
          rate_limit: 60
          cost_per_1k_input: 0.01
          cost_per_1k_output: 0.03
        gpt35_turbo:
          model: "gpt-3.5-turbo"
          api_key: "${OPENAI_API_KEY}"
          temperature: 0.0
          max_tokens: 150
          timeout: 30
          rate_limit: 60
          cost_per_1k_input: 0.0015
          cost_per_1k_output: 0.002

      # Prompt configurations
      prompts:
        zero_shot: |
          Evaluate the factual consistency between this source document and summary on a scale of 0.0 to 1.0:
          
          SOURCE: {source}
          SUMMARY: {summary}
          
          Respond with:
          SCORE: [0.0-1.0]
          CONFIDENCE: [0.0-1.0]
          EXPLANATION: [Brief explanation]

        few_shot: |
          Evaluate factual consistency (0.0-1.0 scale). Examples:
          
          SOURCE: "The meeting was at 3 PM with 5 people."
          SUMMARY: "5 people met at 3 PM."
          SCORE: 1.0
          
          SOURCE: "Revenue increased 10% this quarter."
          SUMMARY: "Revenue decreased 5% this quarter."
          SCORE: 0.0
          
          Now evaluate:
          SOURCE: {source}
          SUMMARY: {summary}
          
          SCORE: [0.0-1.0]
          CONFIDENCE: [0.0-1.0]
          EXPLANATION: [Brief explanation]

    # Anthropic models
    anthropic:
      enabled: true
      models:
        claude_haiku:
          model: "claude-3-haiku-20240307"
          api_key: "${ANTHROPIC_API_KEY}"
          max_tokens: 150
          timeout: 30
          rate_limit: 50
        claude_sonnet:
          model: "claude-3-sonnet-20240229"
          api_key: "${ANTHROPIC_API_KEY}"
          max_tokens: 150
          timeout: 30
          rate_limit: 50
        claude_opus:
          model: "claude-3-opus-20240229"
          api_key: "${ANTHROPIC_API_KEY}"
          max_tokens: 150
          timeout: 30
          rate_limit: 20

      prompt_template: |
        Please evaluate the factual consistency between this source document and summary.
        
        Your task is to determine how well the summary preserves the factual information from the source document without introducing errors, contradictions, or hallucinations.
        
        SOURCE DOCUMENT:
        {source}
        
        SUMMARY:
        {summary}
        
        Evaluation Scale:
        - 1.0: Perfect factual consistency - all facts correctly preserved
        - 0.8: Minor issues - 1-2 small factual discrepancies
        - 0.6: Moderate issues - some important facts missing or incorrect
        - 0.4: Major issues - multiple factual errors or contradictions
        - 0.2: Poor - predominantly incorrect information
        - 0.0: Completely inconsistent or fabricated information
        
        Provide your response in this exact format:
        SCORE: [0.0-1.0]
        CONFIDENCE: [0.0-1.0]
        EXPLANATION: [Brief explanation of your assessment]

    # Qwen models (local)
    qwen:
      enabled: true
      models:
        qwen2_7b:
          model: "Qwen/Qwen2.5-7B-Instruct"
          device: "auto"
          max_new_tokens: 150
          temperature: 0.0
          do_sample: false
          torch_dtype: "float16"
          trust_remote_code: true
        qwen2_14b:
          model: "Qwen/Qwen2.5-14B-Instruct"
          device: "auto"
          max_new_tokens: 150
          temperature: 0.0
          do_sample: false
          torch_dtype: "float16"
          trust_remote_code: true

      system_prompt: |
        You are an expert evaluator tasked with assessing the factual consistency between a source document and its summary. 
        
        Your task is to determine how well the summary preserves the factual information from the source document without introducing errors, contradictions, or hallucinations.
        
        Evaluation Scale:
        - 1.0: Perfect factual consistency - all facts correctly preserved
        - 0.8: Minor issues - 1-2 small factual discrepancies
        - 0.6: Moderate issues - some important facts missing or incorrect
        - 0.4: Major issues - multiple factual errors or contradictions
        - 0.2: Poor - predominantly incorrect information
        - 0.0: Completely inconsistent or fabricated information
        
        Provide your response in this exact format:
        SCORE: [0.0-1.0]
        CONFIDENCE: [0.0-1.0]
        EXPLANATION: [Brief explanation of your assessment]

      user_prompt: |
        Please evaluate the factual consistency between this source document and summary:
        
        SOURCE DOCUMENT:
        {source}
        
        SUMMARY:
        {summary}
        
        Evaluate how well the summary preserves the factual information from the source document.

    # Ensemble configuration
    ensemble:
      enabled: true
      methods:
        weighted_average:
          weights:
            openai_gpt4: 0.4
            anthropic_claude_sonnet: 0.3
            qwen2_7b: 0.3
        voting:
          threshold: 0.5  # Binary classification threshold
        confidence_weighted:
          min_confidence: 0.3  # Minimum confidence to include in ensemble

# Path configurations
paths:
  data_dir: "./data"
  raw_data_dir: "./data/raw"
  processed_data_dir: "./data/processed"
  cache_dir: "./data/cache"
  results_dir: "./results"
  metrics_dir: "./results/metrics"
  correlations_dir: "./results/correlations"
  figures_dir: "./results/figures"
  logs_dir: "./logs"
  config_dir: "./config"
  human_evaluation_dir: "./human_evaluation"
  thesis_dir: "./thesis"

# Experiment configurations
experiments:
  baseline_comparison:
    name: "baseline_comparison"
    description: "Compare FactCC and ROUGE on standard datasets"
    datasets: ["cnn_dailymail", "xsum"]
    metrics: ["factcc", "rouge"]
    output_dir: "${RESULTS_DIR:./results}/baseline"
    random_seed: 42
    save_detailed_results: true

  comprehensive_evaluation:
    name: "comprehensive_evaluation"
    description: "Full evaluation with all metrics"
    datasets: ["cnn_dailymail", "xsum", "summeval"]
    metrics: ["factcc", "qags", "feqa", "rouge", "bertscore"]
    output_dir: "${RESULTS_DIR:./results}/comprehensive"
    random_seed: 42
    save_detailed_results: true

  llm_evaluation:
    name: "llm_evaluation"
    description: "LLM-based factuality evaluation"
    datasets: ["cnn_dailymail", "xsum"]
    metrics: ["llm"]
    llm_models: ["openai_gpt4", "anthropic_claude_sonnet", "qwen2_7b"]
    sample_size: 300  # Smaller sample due to API costs
    output_dir: "${RESULTS_DIR:./results}/llm"
    random_seed: 42
    use_ensemble: true

  human_correlation:
    name: "human_correlation"
    description: "Correlation analysis with human judgments"
    datasets: ["summeval"]  # Has human annotations
    metrics: ["factcc", "qags", "rouge", "llm"]
    output_dir: "${RESULTS_DIR:./results}/human_correlation"
    statistical_tests: ["pearson", "spearman", "kendall"]
    significance_level: 0.05

  domain_analysis:
    name: "domain_analysis"
    description: "Analysis across different domains and datasets"
    datasets: ["cnn_dailymail", "xsum", "multi_news"]
    metrics: ["factcc", "qags", "rouge"]
    output_dir: "${RESULTS_DIR:./results}/domain_analysis"
    stratify_by: ["dataset", "summary_length", "compression_ratio"]

# Human evaluation configuration
human_evaluation:
  sample_size: 300
  num_annotators: 2
  annotation_batches: 6  # Split into manageable batches

  # Stratification strategy
  stratification:
    by_dataset: true
    by_length: true
    by_compression_ratio: true
    by_factuality_score: true  # Include examples across score ranges

  # Annotation interface
  interface:
    type: "streamlit"
    save_progress: true
    randomize_order: true
    include_context: true
    time_limit_minutes: 60  # Per batch

  # Annotation scale
  factuality_scale:
    type: "likert"
    min: 1
    max: 5
    labels:
      1: "Completely inconsistent - major factual errors"
      2: "Mostly inconsistent - multiple factual errors"
      3: "Partially consistent - some factual issues"
      4: "Mostly consistent - minor factual issues"
      5: "Completely consistent - all facts preserved"

  # Quality control
  quality_control:
    include_gold_standards: true
    gold_standard_ratio: 0.1
    min_agreement_threshold: 0.7  # Krippendorff's alpha
    exclude_low_agreement: true

  output_dir: "${RESULTS_DIR:./results}/human_evaluation"
  guidelines_file: "human_evaluation/guidelines.md"

# Statistical analysis configuration
statistical_analysis:
  correlation_methods: ["pearson", "spearman", "kendall"]
  significance_level: 0.05
  multiple_comparisons_correction: "bonferroni"
  bootstrap_iterations: 1000
  confidence_intervals: 0.95

  # Effect size measures
  effect_sizes: ["cohens_d", "hedges_g"]

  # Robustness tests
  robustness_tests:
    cross_validation_folds: 5
    permutation_tests: true
    permutation_iterations: 1000

# Evaluation settings
evaluation:
  random_seed: 42
  reproducibility:
    save_random_state: true
    log_git_commit: true
    save_environment_info: true

  # Performance settings
  batch_processing:
    default_batch_size: 16
    adaptive_batching: true  # Adjust based on available memory
    max_memory_usage: "8GB"
    num_workers: 4
    prefetch_factor: 2

  # Caching settings
  caching:
    enabled: true
    cache_predictions: true
    cache_embeddings: false  # Large memory usage
    cache_llm_responses: true
    cache_expiry_days: 30

  # Output settings
  output:
    save_predictions: true
    save_probabilities: true
    save_explanations: true
    export_formats: ["csv", "json", "parquet"]
    create_visualizations: true
    generate_reports: true

# Logging configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  console_output: true
  file_output: true
  json_format: true
  log_dir: "${LOGS_DIR:./logs}"
  max_file_size: "100MB"
  backup_count: 5

  # Experiment tracking
  experiment_tracking:
    enabled: true
    track_metrics: true
    track_parameters: true
    track_artifacts: true
    log_frequency: 100  # Log every N examples

  # Logger-specific levels
  loggers:
    transformers: "WARNING"
    datasets: "WARNING"
    urllib3: "WARNING"
    requests: "WARNING"
    matplotlib: "WARNING"
    PIL: "WARNING"

# Resource management
resources:
  # Memory management
  memory:
    max_memory_per_process: "8GB"
    memory_monitoring: true
    auto_cleanup: true
    garbage_collection_threshold: 0.8

  # CPU settings
  cpu:
    max_cpu_cores: null  # null for auto-detect
    cpu_affinity: null   # null for no affinity

  # GPU settings
  gpu:
    gpu_memory_fraction: 0.8
    allow_growth: true
    mixed_precision: true  # Use fp16 when possible
    gradient_checkpointing: false

  # Cache settings
  cache:
    max_cache_size: "5GB"
    cache_cleanup_threshold: 0.9
    auto_cleanup_old_cache: true

# API configurations
api:
  # Rate limiting
  rate_limiting:
    enabled: true
    global_rate_limit: 100  # requests per minute
    per_model_limits: true
    exponential_backoff: true
    max_retries: 3

  # Request settings
  requests:
    timeout: 30  # seconds
    retry_delay: 1.0  # seconds
    max_concurrent: 5

  # Cost tracking
  cost_tracking:
    enabled: true
    track_token_usage: true
    daily_budget_limit: 50.0  # USD
    warning_threshold: 0.8  # Warn at 80% of budget

# Visualization settings
visualization:
  style: "seaborn-v0_8-darkgrid"
  figure_size: [10, 6]
  dpi: 300
  save_formats: ["png", "pdf", "svg"]
  color_palette: "husl"
  font_size: 12

  # Plot types
  plots:
    distributions: true
    correlations: true
    comparisons: true
    time_series: false
    scatter_plots: true
    box_plots: true
    violin_plots: true

  # Interactive plots
  interactive:
    enabled: true
    backend: "plotly"
    save_html: true

# Quality assurance
quality_assurance:
  # Testing
  testing:
    run_unit_tests: true
    run_integration_tests: true
    test_coverage_threshold: 0.8
    performance_benchmarks: true

  # Validation
  validation:
    validate_inputs: true
    validate_outputs: true
    schema_validation: true
    range_checks: true

  # Reproducibility
  reproducibility:
    check_deterministic: true
    save_checksums: true
    version_tracking: true

# Development settings
development:
  debug_mode: false
  profile_performance: false
  save_intermediate_results: true
  verbose_logging: false

  # Development data
  dev_data:
    use_small_samples: true
    sample_sizes:
      cnn_dailymail: 50
      xsum: 50
      summeval: 20

  # Feature flags
  features:
    enable_new_metrics: false
    enable_experimental_models: false
    enable_advanced_analysis: true

# Environment-specific overrides
environments:
  development:
    datasets:
      cnn_dailymail:
        sample_size: 50
      xsum:
        sample_size: 50
    logging:
      level: "DEBUG"
    resources:
      memory:
        max_memory_per_process: "4GB"
    api:
      cost_tracking:
        daily_budget_limit: 5.0

  testing:
    datasets:
      cnn_dailymail:
        sample_size: 10
      xsum:
        sample_size: 10
    logging:
      level: "WARNING"
    quality_assurance:
      testing:
        run_unit_tests: true
        run_integration_tests: true

  production:
    datasets:
      cnn_dailymail:
        sample_size: null  # Full dataset
      xsum:
        sample_size: null
    logging:
      level: "INFO"
    resources:
      memory:
        max_memory_per_process: "16GB"
    api:
      cost_tracking:
        daily_budget_limit: 100.0

# Thesis-specific settings
thesis:
  # Requirements
  requirements:
    word_count_target: 8000
    figure_count_min: 10
    table_count_min: 5
    reference_count_min: 30

  # Output generation
  output:
    generate_latex_tables: true
    generate_bibliography: true
    auto_figure_captions: true
    include_appendices: true

  # Analysis depth
  analysis:
    statistical_rigor: "high"
    include_significance_tests: true
    include_effect_sizes: true
    include_confidence_intervals: true