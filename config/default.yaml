# ChatGPT Factuality Evaluation System - Complete Configuration
# MSc AI Thesis Project - University of Manchester 2025
# Student: Michael Ogunjimi | Supervisor: Sophia Ananiadou

# ===== PROJECT METADATA =====
project:
  name: "chatgpt-factuality-eval-thesis"
  version: "1.0.0"
  author: "Michael Ogunjimi"
  institution: "University of Manchester"
  supervisor: "Sophia Ananiadou"
  course: "MSc AI"
  thesis_deadline: "2025-08-31"
  description: "Zero-shot ChatGPT evaluation for factual consistency in text summarization"

# ===== GLOBAL SETTINGS =====
global:
  seed: 42
  debug: false
  log_level: "INFO"
  device: "auto" # auto, cpu, cuda
  num_workers: 4
  cache_enabled: true

# ===== PATHS CONFIGURATION =====
paths:
  # Data directories
  data_dir: "./data"
  raw_data_dir: "./data/raw"
  processed_data_dir: "./data/processed"
  cache_dir: "./data/cache"

  # Results directories
  results_dir: "./results"
  experiments_dir: "./results/experiments"
  figures_dir: "./results/figures"
  tables_dir: "./results/tables"
  logs_dir: "./results/logs"

  # Prompts and configuration
  prompts_dir: "./prompts"
  config_dir: "./config"

  # Models and checkpoints
  models_dir: "./models"
  checkpoints_dir: "./checkpoints"

# ===== CORE TASKS CONFIGURATION =====
tasks:
  # Task 1: Binary Entailment Inference
  entailment_inference:
    enabled: true
    name: "Entailment Inference"
    description: "Binary classification of factual consistency (ENTAILMENT vs CONTRADICTION)"

    # Task parameters
    output_format: "binary"
    valid_labels: ["ENTAILMENT", "CONTRADICTION"]
    default_label: "CONTRADICTION" # Conservative default

    # Evaluation metrics
    metrics:
      primary: ["accuracy", "f1_score"]
      secondary: ["precision", "recall", "auc_roc", "confusion_matrix"]

    # Task-specific settings
    include_confidence: true
    require_explanation: true
    max_examples_per_experiment: 1000

  # Task 2: Summary Ranking
  summary_ranking:
    enabled: true
    name: "Summary Ranking"
    description: "Rank multiple summaries by factual consistency"

    # Task parameters
    output_format: "ranked_list"
    min_summaries: 2
    max_summaries: 5
    default_num_summaries: 3

    # Evaluation metrics
    metrics:
      primary: ["kendall_tau", "spearman_rho"]
      secondary: ["ndcg", "map", "ranking_accuracy"]

    # Task-specific settings
    include_confidence: true
    require_explanation: true
    handle_ties: true
    max_examples_per_experiment: 500

  # Task 3: Consistency Rating
  consistency_rating:
    enabled: true
    name: "Consistency Rating"
    description: "Quantitative rating of factual consistency (0.0-1.0 scale)"

    # Task parameters
    output_format: "float"
    scale_min: 0.0
    scale_max: 1.0
    precision: 2
    default_rating: 0.5

    # Rating scale definitions
    scale_labels:
      1.0: "Perfect factual consistency"
      0.8: "Minor factual issues"
      0.6: "Some factual errors"
      0.4: "Multiple factual errors"
      0.2: "Major factual errors"
      0.0: "Completely inconsistent"

    # Evaluation metrics
    metrics:
      primary: ["pearson_correlation", "spearman_correlation"]
      secondary: ["mae", "rmse", "r_squared"]

    # Task-specific settings
    include_confidence: true
    require_explanation: true
    max_examples_per_experiment: 1000

# ===== OPENAI/CHATGPT CONFIGURATION =====
openai:
  # API key and authentication
  api_key: "${OPENAI_API_KEY}" # Set in environment variable
  # Model settings
  models:
    primary: "gpt-4.1-mini"
    fallbacks: ["o1-mini", "gpt-4o"]

  # API configuration
  api:
    base_url: "https://api.openai.com/v1"
    timeout: 120
    max_retries: 5
    retry_delay: 5.0
    backoff_factor: 2.5

  # Rate limiting (optimized for Tier 1: 500 RPM, 200K TPM, 10K RPD)
  rate_limits:
    requests_per_minute: 450 # 500 RPM limit, use 450 for safety margin
    tokens_per_minute: 180000 # 200K TPM limit, use 180K for safety margin
    requests_per_day: 9500 # 10K RPD limit, use 9500 for safety margin
    concurrent_requests: 10 # Higher concurrency for Tier 1

  # Generation parameters (optimized for GPT-4.1 Mini)
  generation:
    temperature: 0.0 # Deterministic for evaluation consistency
    max_tokens: 2048 # Increased for GPT-4.1 Mini's capabilities
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
    stop_sequences: []

  # Cost management (optimized for GPT-4.1 Mini pricing)
  cost_control:
    total_budget: 200.0 # Total budget in USD (sufficient for full thesis)
    daily_budget: 50.0 # Daily spending limit
    warning_threshold: 0.8 # Warn at 80% of budget
    hard_limit: true # Stop at budget limit
    track_usage: true
    save_usage_logs: true
    cost_per_1k_tokens:
      gpt-4.1-mini:
        input: 0.0004 # $0.40 per 1M tokens
        output: 0.0016 # $1.60 per 1M tokens
      o1-mini:
        input: 0.003 # $3.00 per 1M tokens
        output: 0.012 # $12.00 per 1M tokens
      gpt-4o:
        input: 0.0025 # $2.50 per 1M tokens
        output: 0.01 # $10.00 per 1M tokens

# ===== PROMPT CONFIGURATION =====
prompts:
  # Prompt types
  types: ["zero_shot", "chain_of_thought"]

  # Template settings
  templates:
    directory: "./prompts"
    encoding: "utf-8"
    validate_format: true
    max_length: 8000 # Maximum prompt length in tokens

  # Prompt engineering settings
  engineering:
    include_task_definition: true
    include_examples: false # Pure zero-shot evaluation
    include_confidence_request: true
    include_explanation_request: true
    include_output_format: true

  # Chain-of-thought specific settings
  chain_of_thought:
    reasoning_steps:
      ["extract_facts", "compare_claims", "identify_errors", "make_judgment"]
    step_labels:
      [
        "Key facts in source",
        "Claims in summary",
        "Fact checking",
        "Final judgment",
      ]
    require_structured_output: true

# ===== DATASETS CONFIGURATION =====
datasets:
  # CNN/DailyMail Dataset
  cnn_dailymail:
    enabled: true
    name: "CNN/DailyMail"
    huggingface_name: "ccdv/cnn_dailymail"
    version: "3.0.0"

    # Data settings
    splits: ["test"]
    sample_sizes:
      development: 50 # For testing and development
      evaluation: 500 # For main experiments
      full: null # Use full dataset

    # Processing parameters
    preprocessing:
      max_source_length: 1024
      max_summary_length: 256
      min_summary_length: 10
      remove_duplicates: true
      clean_text: true
      normalize_whitespace: true

    # Task-specific data generation
    task_data:
      entailment_inference:
        generate_negatives: true
        negative_ratio: 0.3
        negative_methods: ["entity_swap", "fact_negation", "hallucination"]

      summary_ranking:
        generate_rankings: true
        summaries_per_source: 4
        quality_levels: ["high", "medium", "low", "very_low"]

      consistency_rating:
        annotation_required: true
        rating_scale: [0.0, 1.0]
        inter_annotator_agreement_target: 0.7

  # XSum Dataset
  xsum:
    enabled: true
    name: "XSum"
    huggingface_name: "EdinburghNLP/xsum"

    # Data settings
    splits: ["test"]
    sample_sizes:
      development: 50
      evaluation: 500
      full: null

    # Processing parameters
    preprocessing:
      max_source_length: 1024
      max_summary_length: 128
      min_summary_length: 8
      remove_duplicates: true
      clean_text: true
      normalize_whitespace: true

    # Task-specific data generation
    task_data:
      entailment_inference:
        generate_negatives: true
        negative_ratio: 0.3
        negative_methods: ["entity_swap", "fact_negation"]

      summary_ranking:
        generate_rankings: true
        summaries_per_source: 3
        quality_levels: ["high", "medium", "low"]

      consistency_rating:
        annotation_required: true
        rating_scale: [0.0, 1.0]

# ===== SOTA BASELINES CONFIGURATION =====
baselines:
  # Enable SOTA methods for comparison
  enabled: true

  # FactCC
  factcc:
    enabled: true
    model_name: "manueldeprada/FactCC"
    batch_size: 8
    device: "auto"
    cache_predictions: true

  # BERTScore
  bertscore:
    enabled: true
    model_type: "roberta-large"
    num_layers: 17
    batch_size: 16
    device: "auto"
    cache_predictions: true

  # ROUGE
  rouge:
    enabled: true
    rouge_types: ["rouge1", "rouge2", "rougeL", "rougeLsum"]
    use_stemmer: true
    split_summaries: true

# ===== EVALUATION CONFIGURATION =====
evaluation:
  # Experimental design
  design:
    cross_validation: false # Not applicable for this thesis
    bootstrap_samples: 1000
    confidence_level: 0.95
    significance_level: 0.05
    random_seed: 42

  # Statistical testing
  statistics:
    tests:
      mcnemar_test: true # For binary classification comparison
      wilcoxon_signed_rank: true # For paired comparisons
      mann_whitney_u: true # For independent samples
      chi_square: true # For categorical data

    corrections:
      bonferroni: true # Multiple comparisons correction
      holm: true # Alternative correction method

    effect_sizes:
      cohens_d: true # Effect size for mean differences
      cramers_v: true # Effect size for categorical associations

  # Performance analysis
  analysis:
    calculate_confidence_intervals: true
    perform_error_analysis: true
    generate_confusion_matrices: true
    compute_correlation_matrices: true

  # Human evaluation (when applicable)
  human_evaluation:
    enabled: false # Enable when ready
    sample_size: 200
    num_annotators: 2
    target_agreement: 0.7 # Krippendorff's alpha
    annotation_interface: "custom"

# ===== EXPERIMENT CONFIGURATION =====
experiments:
  # General settings
  general:
    batch_size: 10 # Process examples in batches
    checkpoint_frequency: 50 # Save progress every N examples
    save_intermediate: true # Save intermediate results
    parallel_processing: false # Sequential for cost control

  # Experiment types
  main_experiments:
    # Zero-shot vs Chain-of-Thought comparison
    prompt_comparison:
      enabled: true
      name: "Prompt Type Comparison"
      description: "Compare zero-shot vs chain-of-thought prompts"
      sample_size: 200
      tasks: ["entailment_inference", "summary_ranking", "consistency_rating"]
      datasets: ["cnn_dailymail", "xsum"]

    # Task performance analysis
    task_analysis:
      enabled: true
      name: "Task Performance Analysis"
      description: "Analyze ChatGPT performance across three tasks"
      sample_size: 500
      prompt_types: ["zero_shot", "chain_of_thought"]
      datasets: ["cnn_dailymail", "xsum"]

    # SOTA comparison
    sota_comparison:
      enabled: true
      name: "SOTA Baseline Comparison"
      description: "Compare ChatGPT with traditional metrics"
      sample_size: 300
      baselines: ["factcc", "bertscore", "rouge"]
      tasks: ["entailment_inference", "consistency_rating"]

  # Ablation studies
  ablation_studies:
    prompt_variations:
      enabled: false # Optional advanced analysis
      variations: ["with_examples", "without_confidence", "different_scales"]

    model_comparison:
      enabled: false # Optional if budget allows
      models: ["gpt-4.1-mini", "o1-mini", "gpt-4o"]

# ===== OUTPUT CONFIGURATION =====
output:
  # Results formats
  formats:
    json: true
    csv: true
    excel: true
    pickle: false

  # Visualization
  visualization:
    generate_plots: true
    plot_formats: ["png", "pdf"]
    plot_dpi: 300
    style: "publication"
    color_palette: "husl"

  # Tables
  tables:
    latex: true
    markdown: true
    csv: true
    significant_digits: 3

  # Reports
  reports:
    auto_generate: false # Manual control for thesis
    include_error_analysis: true
    include_cost_analysis: true
    include_statistical_tests: true

# ===== LOGGING CONFIGURATION =====
logging:
  # General logging
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # File logging
  file:
    enabled: true
    filename: "chatgpt_evaluation.log"
    max_size: "10MB"
    backup_count: 5
    rotation: "size"

  # Console logging
  console:
    enabled: true
    level: "INFO"
    colored: true

  # Experiment logging
  experiment:
    log_predictions: true
    log_api_calls: true
    log_costs: true
    log_errors: true
    detailed_timing: true

# ===== REPRODUCIBILITY =====
reproducibility:
  # Seed control
  seeds:
    global_seed: 42
    numpy_seed: 42
    torch_seed: 42
    random_seed: 42

  # Versioning
  tracking:
    save_config: true
    save_code_hash: true
    save_environment: true
    save_dependencies: true

  # Deterministic settings
  deterministic:
    torch_deterministic: true
    cuda_deterministic: true
    openai_deterministic: true # temperature=0.0

# ===== SAFETY AND ETHICS =====
safety:
  # Data protection
  data_protection:
    anonymize_outputs: true
    no_personal_data: true
    secure_storage: true

  # API safety
  api_safety:
    respect_rate_limits: true
    implement_backoff: true
    monitor_costs: true
    emergency_stop: true

  # Content filtering
  content:
    filter_inappropriate: true
    check_bias: true
    log_concerning_outputs: true

# ===== THESIS-SPECIFIC SETTINGS =====
thesis:
  # Timeline and milestones
  timeline:
    week1_target: 100 # Examples evaluated
    week2_target: 300
    week3_target: 600
    week4_target: 1000
    final_target: 1500

  # Results for thesis chapters
  results_needed:
    methodology_examples: 20
    results_main_table: 500
    error_analysis_cases: 50
    discussion_examples: 30

  # Publication requirements
  publication:
    figures_dpi: 300
    table_precision: 3
    statistical_reporting: "apa"
    significance_threshold: 0.05

  # Academic integrity
  integrity:
    cite_all_sources: true
    report_limitations: true
    share_negative_results: true
    provide_reproducibility_info: true
