# ChatGPT Factuality Evaluation System - Streamlined Configuration
# MSc AI Thesis Project - University of Manchester 2025
# Student: Michael Ogunjimi | Supervisor: Sophia Ananiadou

# ===== PROJECT METADATA =====
project:
  name: "chatgpt-factuality-eval-thesis"
  version: "1.0.0"
  author: "Michael Ogunjimi"
  institution: "University of Manchester"
  supervisor: "Sophia Ananiadou"
  course: "MSc AI"
  description: "Zero-shot ChatGPT evaluation for factual consistency in text summarization"

# ===== GLOBAL SETTINGS =====
global:
  seed: 42
  debug: false
  log_level: "INFO"
  device: "auto" # auto, cpu, cuda, mps
  cache_enabled: true

# ===== PATHS CONFIGURATION =====
paths:
  # Data directories
  data_dir: "./data"
  cache_dir: "./data/cache"
  results_dir: "./results"
  figures_dir: "./results/figures"
  prompts_dir: "./prompts"

# ===== CORE TASKS CONFIGURATION =====
tasks:
  # Task 1: Binary Entailment Inference
  entailment_inference:
    enabled: true
    name: "Entailment Inference"
    description: "Binary classification of factual consistency"
    output_format: "binary"
    metrics:
      primary: ["accuracy", "f1_score"]
      secondary: ["precision", "recall"]

  # Task 2: Summary Ranking
  summary_ranking:
    enabled: true
    name: "Summary Ranking"
    description: "Rank multiple summaries by factual consistency"
    output_format: "ranked_list"
    metrics:
      primary: ["kendall_tau", "spearman_rho"]
      secondary: ["ndcg", "ranking_accuracy"]

  # Task 3: Consistency Rating
  consistency_rating:
    enabled: true
    name: "Consistency Rating"
    description: "Rate factual consistency on 0-100 scale"
    output_format: "float"
    scale_min: 0.0
    scale_max: 1.0
    metrics:
      primary: ["pearson_correlation", "spearman_correlation"]
      secondary: ["mae", "rmse"]

# ===== OPENAI CONFIGURATION =====
openai:
  api_key: "${OPENAI_API_KEY}"

  models:
    primary: "gpt-4.1-mini"
    fallbacks: ["gpt-4o-mini", "o1-mini"]

  api:
    base_url: "https://api.openai.com/v1"
    timeout: 120
    max_retries: 5
    retry_delay: 5.0

  rate_limits:
    requests_per_minute: null # Set via model-specific config
    tokens_per_minute: null # Set via model-specific config
    concurrent_requests: 10

  generation:
    temperature: 0.0
    max_tokens: 8192
    top_p: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0

  cost_control:
    total_budget: 200.0
    daily_budget: 50.0
    track_usage: true

# ===== PROMPTS CONFIGURATION =====
prompts:
  templates:
    directory: "./prompts"
    max_length: 8000
    validate_format: true

# ===== DATASETS CONFIGURATION =====
datasets:
  frank:
    enabled: true
    name: "FRANK"
    description: "Human-annotated factuality labels"
    domain: "news"

  summeval:
    enabled: true
    name: "SummEval"
    description: "Human consistency ratings"
    domain: "news"

# ===== BASELINES CONFIGURATION =====
baselines:
  enabled: true

  factcc:
    enabled: true
    model_name: "manueldeprada/FactCC"
    batch_size: 8
    cache_predictions: true

  bertscore:
    enabled: true
    model_type: "roberta-large"
    num_layers: 17
    batch_size: 16
    cache_predictions: true

  rouge:
    enabled: true
    rouge_types: ["rouge1", "rouge2", "rougeL"]
    use_stemmer: true

# ===== EVALUATION CONFIGURATION =====
evaluation:
  statistics:
    confidence_level: 0.95
    significance_level: 0.05

# ===== EXPERIMENTS CONFIGURATION =====
experiments:
  # Global experiment settings
  global:
    enabled: true
    output_dir: "./results/experiments"
    batch_size: 5
    save_intermediate: true

  # Model configurations
  models:
    "gpt-4.1-mini":
      enabled: true
      name: "GPT-4.1-mini"
      provider: "openai"
      model_id: "gpt-4.1-mini"
      api_config:
        base_url: "https://api.openai.com/v1"
        api_key: "${OPENAI_API_KEY}"
        timeout: 120
        max_retries: 5
        use_dynamic_config: true
        tier: "tier2"
      generation_params:
        temperature: 0.0
        max_tokens: 8192
        top_p: 1.0
      cost_tracking: true

    "qwen2.5:7b":
      enabled: true
      name: "Qwen2.5:7b"
      provider: "ollama"
      model_id: "qwen2.5:7b"
      api_config:
        base_url: "${OLLAMA_HOST}/v1"
        api_key: "ollama"
        timeout: 180
        max_retries: 3
      generation_params:
        temperature: 0.0
        max_tokens: 8192
        top_p: 1.0
      cost_tracking: false

    "llama3.1:8b":
      enabled: true
      name: "Llama3.1:8b"
      provider: "ollama"
      model_id: "llama3.1:8b"
      api_config:
        base_url: "${OLLAMA_HOST}/v1"
        api_key: "ollama"
        timeout: 180
        max_retries: 3
      generation_params:
        temperature: 0.0
        max_tokens: 8192
        top_p: 1.0
      cost_tracking: false

  # Multi-LLM Evaluation Experiment
  llm_evaluation:
    enabled: true
    name: "Multi-LLM Factuality Evaluation"
    default_models: ["gpt-4.1-mini", "qwen2.5:7b", "llama3.1:8b"]
    sample_size: 1000
    datasets: ["frank", "summeval"]

    tasks:
      entailment_inference:
        enabled: true
        sample_size: 1000
        metrics: ["accuracy", "f1_score", "precision", "recall"]

      summary_ranking:
        enabled: true
        sample_size: 1000
        metrics: ["kendall_tau", "spearman_rho", "ndcg"]

      consistency_rating:
        enabled: true
        sample_size: 1000
        metrics: ["pearson_correlation", "spearman_correlation", "mae", "rmse"]

    sample_sizes:
      quick_test: 50
      development: 200
      comprehensive: 1000
      full_scale: 2000

  # SOTA Baseline Comparison
  sota_comparison:
    enabled: true
    name: "SOTA Baseline Comparison"
    default_models: ["gpt-4.1-mini"]
    sample_size: 500
    datasets: ["frank", "summeval"]

    baselines:
      factcc:
        enabled: true
        batch_size: 8
      bertscore:
        enabled: true
        batch_size: 16
      rouge:
        enabled: true

    tasks:
      entailment_inference:
        enabled: true
        sample_size: 500
        comparison_metrics: ["accuracy", "f1_score", "cohen_kappa"]

      consistency_rating:
        enabled: true
        sample_size: 500
        comparison_metrics:
          ["pearson_correlation", "spearman_correlation", "mae"]

    sample_sizes:
      quick_test: 25
      development: 100
      comprehensive: 500
      full_scale: 1000
