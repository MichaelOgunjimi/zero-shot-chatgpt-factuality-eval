# Batch Processing System for ChatGPT Factuality Evaluation

**Author**: Michael Ogunjimi  
**Institution**: University of Manchester  
**Course**: MSc AI  
**Date**: July 2025  

## Overview

This batch processing system provides cost-effective large-scale evaluation of ChatGPT's factuality assessment capabilities using OpenAI's Batch API. The system mirrors the functionality of the standard experiments while achieving up to **50% cost savings** through batch processing optimization.

## üéØ Key Features

- **Cost Optimization**: 50% savings vs synchronous API calls
- **Scalable Processing**: Handle thousands of evaluations efficiently
- **Comprehensive Monitoring**: Real-time job status tracking and progress reporting
- **Academic Focus**: Designed specifically for thesis-level research requirements
- **Statistical Rigor**: Complete statistical analysis and significance testing
- **Reproducibility**: Comprehensive logging and configuration management

## üìÅ System Architecture

```
src/
‚îú‚îÄ‚îÄ batch/                           # Core batch processing system
‚îÇ   ‚îú‚îÄ‚îÄ batch_manager.py            # Batch job orchestration and management
‚îÇ   ‚îú‚îÄ‚îÄ batch_monitor.py            # Real-time monitoring and progress tracking
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ llm_clients/
‚îÇ   ‚îî‚îÄ‚îÄ openai_client_batch.py       # Specialized batch processing client
‚îî‚îÄ‚îÄ ...

experiments/batch_processing/       # Batch experiment implementations
‚îú‚îÄ‚îÄ batch_run_all_experiments.py    # Master orchestrator for all experiments
‚îú‚îÄ‚îÄ batch_run_chatgpt_evaluation.py # Main ChatGPT evaluation (batch version)
‚îú‚îÄ‚îÄ batch_run_prompt_comparison.py  # Zero-shot vs CoT comparison (batch version)
‚îú‚îÄ‚îÄ batch_run_sota_comparison.py    # SOTA baseline comparison (batch version)
‚îú‚îÄ‚îÄ __init__.py                     # Package initialization
‚îî‚îÄ‚îÄ README.md                       # This documentation
```

## üöÄ Quick Start

### 1. Prerequisites

Ensure you have the standard factuality evaluation system set up:

```bash
# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.template .env
# Edit .env with your OpenAI API key
```

### 2. Run Quick Test

Test the batch processing system with a small sample:

```bash
# Quick test of individual experiments
python experiments/batch_processing/batch_run_chatgpt_evaluation.py --quick-test
python experiments/batch_processing/batch_run_prompt_comparison.py --quick-test
python experiments/batch_processing/batch_run_sota_comparison.py --quick-test

# Quick test of complete suite
python experiments/batch_processing/batch_run_all_experiments.py --quick-test
```

### 3. Run Full Experiments

Run complete experiments with full sample sizes:

```bash
# Complete experimental suite (recommended for thesis)
python experiments/batch_processing/batch_run_all_experiments.py --full-suite

# Individual experiments with custom parameters
python experiments/batch_processing/batch_run_chatgpt_evaluation.py --model gpt-4o-mini --sample-size 500
python experiments/batch_processing/batch_run_prompt_comparison.py --datasets cnn_dailymail xsum --sample-size 300
python experiments/batch_processing/batch_run_sota_comparison.py --baselines factcc bertscore --sample-size 200
```

## üìä Available Experiments

### 1. ChatGPT Evaluation (`batch_run_chatgpt_evaluation.py`)

**Purpose**: Baseline performance assessment across factuality tasks  
**Tasks**: Entailment Inference, Summary Ranking, Consistency Rating  
**Datasets**: CNN/DailyMail, XSum  
**Duration**: 1-2 hours  
**Est. Cost**: $2-8  

```bash
# Basic usage
python experiments/batch_processing/batch_run_chatgpt_evaluation.py

# Custom configuration
python experiments/batch_processing/batch_run_chatgpt_evaluation.py \
    --model gpt-4o-mini \
    --tier tier2 \
    --tasks entailment_inference consistency_rating \
    --datasets cnn_dailymail \
    --sample-size 500 \
    --prompt-types zero_shot
```

**Key Outputs**:

- Task performance metrics across datasets
- Success rate analysis by task type
- Cost optimization results
- Detailed evaluation report

### 2. Prompt Comparison (`batch_run_prompt_comparison.py`)

**Purpose**: Compare zero-shot vs chain-of-thought prompting strategies  
**Analysis**: Statistical significance testing and effect size calculation  
**Duration**: 2-3 hours  
**Est. Cost**: $4-12  

```bash
# Basic usage
python experiments/batch_processing/batch_run_prompt_comparison.py

# Custom configuration
python experiments/batch_processing/batch_run_prompt_comparison.py \
    --model gpt-4o-mini \
    --tasks entailment_inference summary_ranking \
    --datasets cnn_dailymail xsum \
    --sample-size 300
```

**Key Outputs**:

- Performance improvement analysis with statistical testing
- Effect size calculations (Cohen's h)
- Cost-benefit analysis for prompt strategies
- Interactive comparison visualizations

### 3. SOTA Comparison (`batch_sota_comparison.py`)

**Purpose**: Comprehensive batch processing version of SOTA baseline comparison  
**Baselines**: FactCC, BERTScore, ROUGE  
**Duration**: 1-2 hours  
**Est. Cost**: $2-6  
**Key Features**: Resume capability, detailed batch monitoring, cost optimization

```bash
# Basic usage - all tasks, all datasets, all baselines
python experiments/batch_processing/batch_sota_comparison.py

# Quick test with minimal data
python experiments/batch_processing/batch_sota_comparison.py --quick-test

# Submit batch jobs without waiting (for background processing)
python experiments/batch_processing/batch_sota_comparison.py --no-wait

# Specific task and dataset combination
python experiments/batch_processing/batch_sota_comparison.py \
    --task entailment_inference \
    --dataset cnn_dailymail \
    --baseline factcc

# Custom model and batch configuration
python experiments/batch_processing/batch_sota_comparison.py \
    --model gpt-4o-mini \
    --tier tier2 \
    --batch-size 50 \
    --sample-size 100

# Resume from existing batch jobs
python experiments/batch_processing/batch_sota_comparison.py \
    --resume-from-jobs batch_xyz123 batch_abc456
```

**Command Line Arguments**:

- `--model`: OpenAI model (`gpt-4.1-mini`, `gpt-4o-mini`, `o1-mini`, `gpt-4o`)
- `--tier`: API tier (`tier1` to `tier5`)
- `--task`: Single task (`entailment_inference`, `consistency_rating`)
- `--dataset`: Single dataset (`cnn_dailymail`, `xsum`)
- `--baseline`: Single baseline (`factcc`, `bertscore`, `rouge`)
- `--sample-size`: Examples per dataset (default: 300)
- `--batch-size`: Prompts per batch job (default: 1000)
- `--prompt-type`: Prompt strategy (`zero_shot`, `chain_of_thought`)
- `--no-wait`: Submit without waiting for completion
- `--quick-test`: Test with 20 examples, batch size 10
- `--resume-from-jobs`: Resume from batch job IDs

**Key Outputs**:

- Correlation analysis (Pearson, Spearman, Cohen's Kappa)
- Performance comparison matrices
- Batch processing cost analysis
- Statistical significance testing
- Comprehensive experiment report
- Resume capability for interrupted experiments

### 4. Legacy SOTA Comparison (`batch_run_sota_comparison.py`)

**Purpose**: Original SOTA comparison with additional QAGS baseline  
**Baselines**: FactCC, BERTScore, ROUGE, QAGS  
**Duration**: 1-2 hours  
**Est. Cost**: $2-6  

```bash
# Basic usage
python experiments/batch_processing/batch_run_sota_comparison.py

# Custom configuration
python experiments/batch_processing/batch_run_sota_comparison.py \
    --model gpt-4o-mini \
    --tasks entailment_inference consistency_rating \
    --baselines factcc bertscore rouge \
    --sample-size 400 \
    --prompt-type zero_shot
```

**Key Outputs**:

- Pearson and Spearman correlation analysis
- Method ranking and agreement matrices
- Score distribution comparisons
- Validation against traditional metrics

### 5. Master Suite (`batch_run_all_experiments.py`)

**Purpose**: Complete experimental suite with consolidated analysis  
**Includes**: All three experiments plus master-level analysis  
**Duration**: 3-6 hours  
**Est. Cost**: $8-25  

```bash
# Complete suite (recommended for thesis)
python experiments/batch_processing/batch_run_all_experiments.py --full-suite

# Quick validation
python experiments/batch_processing/batch_run_all_experiments.py --quick-test

# Specific experiments only
python experiments/batch_processing/batch_run_all_experiments.py \
    --experiments chatgpt_evaluation prompt_comparison \
    --model gpt-4o-mini
```

**Key Outputs**:

- Cross-experiment consolidated analysis
- Master performance dashboard
- Comprehensive cost analysis
- Executive summary for thesis inclusion

## ‚öôÔ∏è Configuration Options

### Model Selection

The system supports multiple OpenAI models with automatic tier-based rate limiting:

```bash
# Cost-effective (recommended for large-scale)
--model gpt-4o-mini --tier tier2

# Balanced performance/cost
--model gpt-4.1-mini --tier tier2

# High performance (for critical experiments)
--model gpt-4o --tier tier2

# Reasoning-focused
--model o1-mini --tier tier2
```

### Sample Size Guidelines

| Purpose | Quick Test | Development | Thesis |
|---------|------------|-------------|---------|
| Sample Size | 20-50 | 100-200 | 300-500 |
| Cost | $1-3 | $5-15 | $15-50 |
| Duration | 30 min | 1-2 hours | 2-6 hours |

### Advanced Configuration

```bash
# Custom experiment name
--experiment-name "my_thesis_experiment_v2"

# Specific tasks and datasets
--tasks entailment_inference consistency_rating
--datasets cnn_dailymail

# Prompt type for SOTA comparison
--prompt-type chain_of_thought
```

## üí∞ Cost Management

### Cost Estimation

Use the built-in cost estimation tools:

```python
from experiments.batch_processing import estimate_experiment_cost

# Estimate cost for specific experiment
cost_info = estimate_experiment_cost("batch_master_suite", sample_size=300, model="gpt-4o-mini")
print(f"Estimated cost: ${cost_info['estimated_cost']:.2f}")
print(f"Estimated savings: ${cost_info['estimated_savings']:.2f}")
```

### Batch Processing Savings

| Model | Sync Cost/1K | Batch Cost/1K | Savings |
|-------|--------------|---------------|---------|
| gpt-4o-mini | $0.75 | $0.375 | 50% |
| gpt-4.1-mini | $2.00 | $1.00 | 50% |
| gpt-4o | $12.50 | $6.25 | 50% |

### Budget Planning

For thesis-level experiments:

- **Quick validation**: $10-20
- **Development/testing**: $50-100  
- **Full thesis experiments**: $100-300
- **Comprehensive analysis**: $200-500

## üìà Output Structure

Each experiment generates a comprehensive output structure:

```
results/experiments/batch_processing/[experiment_name]/
‚îú‚îÄ‚îÄ chatgpt_evaluation/                  # ChatGPT evaluation results
‚îÇ   ‚îú‚îÄ‚îÄ figures/                        # Visualization charts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ task_performance_comparison.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cost_analysis.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ success_rate_heatmap.png
‚îÇ   ‚îú‚îÄ‚îÄ logs/                           # Execution logs
‚îÇ   ‚îú‚îÄ‚îÄ batch_chatgpt_evaluation_results.json
‚îÇ   ‚îî‚îÄ‚îÄ experiment_report.md
‚îú‚îÄ‚îÄ prompt_comparison/                   # Prompt comparison results
‚îÇ   ‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ comprehensive_report.md     # Statistical analysis report
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ complete_results.csv        # Raw results data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cost_analysis.csv          # Cost breakdown
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summary_statistics.csv     # Statistical summaries
‚îÇ   ‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ performance_comparison_boxplots.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ statistical_significance_heatmap.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ effect_sizes_heatmap.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ interactive_performance_comparison.html
‚îÇ   ‚îú‚îÄ‚îÄ latex/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ performance_table.tex       # LaTeX tables for thesis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ statistical_table.tex
‚îÇ   ‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ batch_prompt_comparison_results.json
‚îÇ   ‚îî‚îÄ‚îÄ tables/
‚îÇ       ‚îî‚îÄ‚îÄ performance_comparison.csv
‚îú‚îÄ‚îÄ sota_comparison/                     # SOTA comparison results
‚îÇ   ‚îú‚îÄ‚îÄ baseline_results/               # Individual baseline results
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ factcc_results.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bertscore_results.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rouge_results.json
‚îÇ   ‚îú‚îÄ‚îÄ figures/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ correlation_heatmap.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ method_ranking.png
‚îÇ   ‚îú‚îÄ‚îÄ batch_sota_comparison_results.json
‚îÇ   ‚îî‚îÄ‚îÄ sota_comparison_report.md
‚îú‚îÄ‚îÄ master_analysis/                     # Consolidated master analysis
‚îÇ   ‚îú‚îÄ‚îÄ visualizations/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cost_breakdown.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ experimental_timeline.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ key_findings_summary.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ performance_dashboard.png
‚îÇ   ‚îú‚îÄ‚îÄ executive_summary.md            # Quick summary for thesis
‚îÇ   ‚îú‚îÄ‚îÄ master_experimental_report.md   # Comprehensive report
‚îÇ   ‚îî‚îÄ‚îÄ master_experimental_results.json
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îú‚îÄ‚îÄ batch_master_[timestamp].log    # Consolidated execution logs
‚îÇ   ‚îî‚îÄ‚îÄ error.log                       # Error tracking
‚îú‚îÄ‚îÄ batch_master_results.json           # High-level batch processing summary
‚îî‚îÄ‚îÄ README.md                           # Experiment-specific documentation
```

## üîß Advanced Usage

### Programmatic Usage

```python
import asyncio
from experiments.batch_processing import (
    BatchChatGPTEvaluationExperiment,
    BatchPromptComparisonExperiment,
    BatchSOTAComparisonExperiment,
    BatchMasterExperimentRunner
)

# Run individual experiment
async def run_custom_evaluation():
    experiment = BatchChatGPTEvaluationExperiment(
        model="gpt-4o-mini",
        tier="tier2",
        experiment_name="custom_evaluation_v1"
    )
    
    results = await experiment.run_batch_evaluation(
        tasks=['entailment_inference', 'consistency_rating'],
        datasets=['cnn_dailymail'],
        sample_size=200,
        quick_test=False
    )
    
    return results

# Run complete suite
async def run_thesis_experiments():
    master_runner = BatchMasterExperimentRunner(
        model="gpt-4o-mini",
        tier="tier2",
        experiment_name="thesis_final_experiments"
    )
    
    results = await master_runner.run_complete_batch_experimental_suite(
        quick_test=False,
        full_suite=True
    )
    
    return results

# Execute
results = asyncio.run(run_thesis_experiments())
```

### Monitoring and Status Checking

```python
from src.batch import BatchManager, BatchMonitor

# Check batch status
batch_manager = BatchManager()
all_jobs = batch_manager.get_all_jobs()
print(f"Active jobs: {len(all_jobs['active'])}")
print(f"Completed jobs: {len(all_jobs['completed'])}")

# Generate monitoring report
monitor = BatchMonitor(batch_manager)
report = monitor.generate_monitoring_report()
print(report)
```

## üîç Monitoring and Debugging

### Real-Time Monitoring

The system provides comprehensive monitoring capabilities:

1. **Live Status Display**: Real-time job progress with Rich console interface
2. **Cost Tracking**: Continuous cost monitoring and budget alerts  
3. **Health Checks**: System health validation and issue detection
4. **Progress Reporting**: Detailed progress tracking with time estimates

### Log Analysis

Check execution logs for detailed information:

```bash
# View main experiment log
tail -f results/experiments/batch_processing/[experiment_name]/logs/batch_master_[timestamp].log

# Check for errors
cat results/experiments/batch_processing/[experiment_name]/logs/error.log

# Monitor specific job
grep "job_id_here" results/experiments/batch_processing/[experiment_name]/logs/*.log
```

### Common Issues and Solutions

#### 1. Batch Job Failures

**Symptoms**: Jobs stuck in "validating" or "failed" status  
**Solutions**:

- Check API key validity: `echo $OPENAI_API_KEY`
- Verify batch queue limits in configuration
- Review input file format in logs

#### 2. High Costs

**Symptoms**: Costs exceeding estimates  
**Solutions**:

- Use `--quick-test` for development
- Reduce `--sample-size` parameter  
- Switch to `gpt-4o-mini` for cost optimization

#### 3. Long Processing Times

**Symptoms**: Batch jobs taking >24 hours  
**Solutions**:

- Check OpenAI batch queue status
- Consider splitting large jobs into smaller batches
- Monitor system health with health check functions

#### 4. Parsing Failures

**Symptoms**: Low success rates in result parsing  
**Solutions**:

- Review prompt templates for clarity
- Check model compatibility with task requirements
- Examine failed examples in error logs

## üìä Performance Characteristics

### Processing Speed

| Experiment Type | Sample Size | Batch Jobs | Est. Duration |
|----------------|-------------|------------|---------------|
| ChatGPT Eval | 300 | 6 | 1-2 hours |
| Prompt Comparison | 300 | 12 | 2-3 hours |
| SOTA Comparison | 300 | 4 | 1-2 hours |
| Complete Suite | 300 | 22 | 3-6 hours |

### Cost Analysis

| Model | Quick Test (50) | Development (200) | Thesis (500) |
|-------|----------------|-------------------|--------------|
| gpt-4o-mini | $0.50 | $4 | $20 |
| gpt-4.1-mini | $1.50 | $12 | $60 |
| gpt-4o | $8 | $60 | $300 |

*Costs include 50% batch processing discount*

### Success Rates

- **Job Submission**: >99% success rate
- **Batch Processing**: >95% completion rate  
- **Result Parsing**: >90% parsing success
- **Overall Pipeline**: >85% end-to-end success

## üî¨ Research Applications

### Thesis Integration

The batch processing system is specifically designed for academic research:

#### 1. **Methodology Chapter**

- Use batch processing details as cost optimization methodology
- Include processing time analysis for efficiency discussion
- Reference comprehensive error handling for reliability

#### 2. **Results Chapter**

- Use generated tables and figures directly in thesis
- Include statistical analysis from automated reports
- Reference cost analysis for practical considerations

#### 3. **Implementation Chapter**

- Detailed system architecture documentation
- Batch processing as scalability solution
- Reproducibility through comprehensive logging

### Publication-Ready Outputs

All visualizations and tables are generated in publication quality:

- **Figures**: High-resolution PNG (300 DPI) and interactive HTML
- **Tables**: CSV, LaTeX, and formatted markdown
- **Statistics**: Complete significance testing and effect sizes
- **Reports**: Structured markdown suitable for thesis inclusion

## üìã Configuration Reference

### Default Configuration

The system uses model-specific configurations from `config/models/`:

```yaml
# Example: gpt-4o-mini_tier2.yaml
openai:
  models:
    primary: "gpt-4o-mini"
    fallbacks: ["gpt-4.1-mini", "o1-mini"]
  
  rate_limits:
    requests_per_minute: 1800
    tokens_per_minute: 1800000
    batch_queue_limit: 1800000
  
  batch:
    enabled: true
    max_queue_size: 1800000
    processing_timeout: 86400
    cost_savings: 0.5
```

### Environment Variables

```bash
# Required
OPENAI_API_KEY=your-api-key-here

# Optional
FACTUALITY_ENV=production
CACHE_DIR=./data/cache
BATCH_SIZE=16
DEVICE=auto
```

### Experiment-Specific Configurations

#### Quick Test Settings

- Sample size: 20-50 examples
- Limited tasks/datasets
- Fast validation (~30 minutes)

#### Development Settings  

- Sample size: 100-200 examples
- Selected tasks/datasets
- Moderate validation (~1-2 hours)

#### Production Settings

- Sample size: 300-500 examples
- All tasks/datasets
- Full validation (~3-6 hours)

## üõ†Ô∏è Troubleshooting

### Diagnostic Commands

```bash
# Check system health
python -c "
import asyncio
from src.batch import BatchManager, BatchMonitor
async def health_check():
    manager = BatchManager()
    monitor = BatchMonitor(manager)
    health = await monitor.health_check()
    print(health)
asyncio.run(health_check())
"

# View active jobs
python -c "
from src.batch import BatchManager
manager = BatchManager()
jobs = manager.get_all_jobs()
print(f'Active: {len(jobs[\"active\"])}')
print(f'Completed: {len(jobs[\"completed\"])}')
print(f'Failed: {len(jobs[\"failed\"])}')
"

# Check cost summary
python -c "
from src.batch import BatchManager
manager = BatchManager()
cost_summary = manager.get_cost_summary()
print(f'Total cost: ${cost_summary[\"total_actual_cost\"]:.4f}')
print(f'Savings: ${cost_summary[\"total_savings\"]:.4f}')
"
```

### Common Error Messages

#### `OPENAI_API_KEY environment variable not set`

**Solution**: Set your API key in `.env` file or export as environment variable

#### `Batch processing is not available or enabled`

**Solution**: Check batch configuration in model config files

#### `Batch size exceeds maximum queue size`

**Solution**: Reduce sample size or split into multiple smaller experiments

#### `Failed to download results for job`

**Solution**: Check job completion status and retry after batch processing completes

## üìö API Reference

### Batch Manager

```python
from src.batch import BatchManager

manager = BatchManager(config, experiment_name)

# Submit batch job
job = await manager.submit_batch_job(requests, task_type, dataset_name, prompt_type)

# Monitor status
status = await manager.get_batch_status(job_id)

# Download results
results = await manager.download_batch_results(job)
```

### Batch Monitor

```python
from src.batch import BatchMonitor

monitor = BatchMonitor(batch_manager, update_interval=60)

# Start monitoring
await monitor.start_monitoring(job_ids)

# Wait for completion
completed = await monitor.wait_for_all_completion(job_ids)

# Generate report
report = monitor.generate_monitoring_report()
```

### Batch Client

```python
from src.llm_clients.openai_client_batch import OpenAIBatchClient

client = OpenAIBatchClient(config, experiment_name)

# Process experiment batch
job, results = await client.process_factuality_evaluation_batch(
    formatted_prompts, task_type, dataset_name, prompt_type
)

# Get cost estimate
cost_info = client.estimate_batch_cost(formatted_prompts)
```

## üéì Academic Guidelines

### For MSc/PhD Research

1. **Ethical Considerations**: Ensure API usage complies with university policies
2. **Cost Management**: Plan budget allocation for different experiment scales
3. **Time Planning**: Allow 24-48 hours for batch processing completion
4. **Reproducibility**: Archive all configuration files and logs
5. **Documentation**: Use generated reports as foundation for thesis chapters

### For Publication

1. **Methodology**: Reference batch processing as cost optimization technique
2. **Limitations**: Discuss 24-hour processing delay vs real-time evaluation
3. **Reproducibility**: Provide complete configuration and code availability
4. **Cost Analysis**: Include cost-effectiveness discussion for practical adoption

### Citation

If using this batch processing system in academic work:

```bibtex
@misc{ogunjimi2025batch,
  title={Batch Processing System for ChatGPT Factuality Evaluation},
  author={Ogunjimi, Michael},
  year={2025},
  institution={University of Manchester},
  note={MSc AI Thesis Implementation}
}
```

## üîÑ Version History

### v1.0.0 (July 2025)

- Initial implementation of complete batch processing system
- Full integration with OpenAI Batch API
- Comprehensive monitoring and cost tracking
- Publication-quality visualization and reporting
- Statistical analysis with significance testing
- Master experiment orchestration

## ü§ù Contributing

This system was developed for academic research at University of Manchester. For questions or issues:

1. **Check logs**: Review execution and error logs first
2. **Configuration**: Verify model and API configurations
3. **Documentation**: Consult this README and generated reports
4. **Testing**: Use quick test mode for validation

## üìû Support

For technical issues related to this batch processing system:

- **Author**: Michael Ogunjimi
- **Institution**: University of Manchester
- **Course**: MSc AI  
- **Email**: <michael.ogunjimi@postgrad.manchester.ac.uk>

For OpenAI API issues:

- **Documentation**: <https://docs.openai.com/>
- **Support**: <https://help.openai.com/>

---

*This batch processing system was developed as part of an MSc AI thesis at the University of Manchester, focusing on cost-effective large-scale evaluation of ChatGPT's factuality assessment capabilities.*
